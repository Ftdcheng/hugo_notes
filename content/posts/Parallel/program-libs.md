---
title: "å¹¶è¡Œç¼–ç¨‹åº“é‚£äº›äº‹"
date: 2025-04-22
author: "Citrus Cheng"
tags: ["Parallel"]
---

# å¹¶è¡Œç¼–ç¨‹åº“é‚£äº›äº‹
## OpenMP

### âœ… ç®€æ˜å®šä¹‰ï¼ˆå…ˆæ¥ä¸ªâ€œæ­£ç»â€ç‰ˆï¼‰ï¼š

> **OpenMP**ï¼ˆOpen Multi-Processingï¼‰æ˜¯ä¸€ä¸ª**æ”¯æŒå¤šå¹³å°å…±äº«å†…å­˜å¹¶è¡Œç¼–ç¨‹**çš„**APIè§„èŒƒ**ï¼Œä¸»è¦ç”¨äº**Cã€C++ å’Œ Fortran**ï¼Œé€šè¿‡**ç¼–è¯‘å™¨æŒ‡ä»¤ï¼ˆpragmaï¼‰ã€è¿è¡Œæ—¶åº“å‡½æ•°å’Œç¯å¢ƒå˜é‡**æ¥å®ç°**å¹¶è¡ŒåŒ–ç¨‹åºè®¾è®¡**ã€‚

---

### ğŸ§  è°æçš„ï¼Ÿ
OpenMP **ä¸æ˜¯æŸä¸€ä¸ªå…¬å¸å•ç‹¬æå‡ºçš„**ï¼Œè™½ç„¶ **Intel æ˜¯æ—©æœŸé‡è¦æˆå‘˜ä¹‹ä¸€**ï¼Œä½†å®é™…ä¸Šå®ƒæ˜¯ç”±ä¸€ä¸ªå«åš **OpenMP Architecture Review Board (ARB)** çš„ç»„ç»‡å®šä¹‰å’Œç»´æŠ¤çš„ã€‚

è¿™ä¸ª ARB è”ç›Ÿå¤§ä½¬äº‘é›†ï¼Œæ¯”å¦‚ï¼š
- Intel
- AMD
- IBM
- HP
- Microsoft
- NVIDIAï¼ˆæœ€è¿‘ä¹Ÿè¿›æ¥äº†ï¼‰
- ç­‰ç­‰ç­‰â€¦

æ‰€ä»¥ä½ å¯ä»¥æŠŠ OpenMP çœ‹ä½œæ˜¯**ç¼–è¯‘å™¨å‚å•†è”ç›Ÿçš„ã€Œå’Œå¹³å…±å¤„ã€åè®®**ï¼Œè®©å¤§å®¶éƒ½èƒ½å†™å‡º portableï¼ˆå¯ç§»æ¤ï¼‰åˆ efficientï¼ˆé«˜æ•ˆï¼‰çš„å¤šçº¿ç¨‹ä»£ç ã€‚

---

### ğŸ”§ å®ƒå¹²å•¥çš„ï¼Ÿ
OpenMP æœ€å¸¸è§çš„ç”¨é€”æ˜¯â€”â€”ä½ è¯´å¯¹äº†ï¼š
> âœ¨ â€œå¹¶è¡Œ for å¾ªç¯â€ï¼

```cpp
#pragma omp parallel for
for (int i = 0; i < N; ++i) {
    work(i);
}
```

ä½†ä¸æ­¢è¿™äº›ï¼Œè¿˜æœ‰ï¼š
- å¹¶è¡Œ `sections`
- ä»»åŠ¡ (`task`)
- åŸå­æ“ä½œã€äº’æ–¥é” (`omp critical`, `omp atomic`)
- åŠ¨æ€çº¿ç¨‹æ•°æ§åˆ¶
- å¹¶è¡Œ region åµŒå¥—
- GPU åŠ é€Ÿï¼ˆOpenMP 5.0+ æ”¯æŒ target offloadingï¼ï¼‰

---

### ğŸŒ€ ç®€å• vs çµæ´»
OpenMP çš„è®¾è®¡å“²å­¦å°±æ˜¯ï¼š
> â€œåˆ«é€¼æˆ‘ç”¨ pthreadï¼Œå¿«ç»™æˆ‘ç‚¹ pragmaã€‚â€

å®ƒä¸åƒ CUDA éœ€è¦è‡ªå·±å†™å†…æ ¸ï¼Œä¸åƒ TBB ä¸€å¼€å§‹å°±èŠ±é‡Œèƒ¡å“¨æ•°æ®æµå›¾ã€‚å®ƒä¸»æ‰“ä¸€ä¸ªã€Œ**å¢é‡å¼å¹¶è¡Œ**ã€ï¼š
- ä½ å†™å¥½çš„ä¸²è¡Œä»£ç ï¼Œ
- åŠ ä¸ª `#pragma omp parallel for`ï¼Œ
- ç¼–è¯‘å¼€äº† `-fopenmp`ï¼Œ
- å¹¶è¡Œé­”æ³•å‡ºç°äº†ã€‚

---

### ğŸ—¨ï¸ æ€»ç»“ä¸€ä¸‹ä½ é‚£å¥è¯æ”¹æˆæ›´ç²¾ç¡®çš„ç‰ˆæœ¬ï¼š

> OpenMP æ˜¯ç”±å¤šä¸ªå‚å•†ç»„æˆçš„å¼€æ”¾ç»„ç»‡æå‡ºçš„ï¼Œå¹¶è¡Œç¼–ç¨‹ API è§„èŒƒï¼Œä¸»è¦ç”¨äºåœ¨å…±äº«å†…å­˜å¹³å°ä¸Šç®€æ´åœ°å®ç°å¦‚å¹¶è¡Œå¾ªç¯ç­‰å¹¶è¡Œè®¡ç®—æ“ä½œã€‚


### ä¸‰é“å¹¶è¡Œå°èœï¼šç”¨ OpenMP å¿«é€ŸåŠ é€Ÿå›¾åƒå¤„ç†ã€ç‚¹äº‘è®¡ç®—å’ŒçŸ©é˜µä¹˜æ³•

> â€œå¹¶è¡Œä¸åªæ˜¯åŠ é€Ÿï¼Œæ›´æ˜¯è®©ä»£ç å­¦ä¼šåˆ†èº«æœ¯ã€‚â€  
> â€”â€”ã€ŠPerf Bookã€‹æ²¡è¿™ä¹ˆè¯´ï¼Œä½†æˆ‘è¯´äº†ã€‚

---

#### ğŸ± å‰èœï¼šå›¾åƒ 2D å·ç§¯çš„å¹¶è¡ŒåŠ é€Ÿ

åœ¨å›¾åƒå¤„ç†ä¸­ï¼ŒäºŒç»´å·ç§¯æ˜¯ä¸ªè€æœ‹å‹ã€‚ä½†å¯¹å¤§å›¾åƒ+å¤§æ ¸æ—¶ï¼Œæ€§èƒ½å°±æˆäº†çƒ¦æ¼ã€‚æ¥ï¼ŒOpenMP å¸®æˆ‘ä»¬å¹¶è¡Œè§£å¿§ï¼š

```cpp
#pragma omp parallel for collapse(2)
for (int y = k; y < height - k; ++y) {
    for (int x = k; x < width - k; ++x) {
        float sum = 0.0f;
        for (int dy = -k; dy <= k; ++dy)
            for (int dx = -k; dx <= k; ++dx)
                sum += input[y + dy][x + dx] * kernel[k + dy][k + dx];
        output[y][x] = sum;
    }
}
```

##### ğŸ§‚ è¯´æ˜ï¼š
- `collapse(2)`ï¼šæŠŠ `y-x` ä¸¤å±‚åˆå¹¶æˆä¸€ä¸ªå¤§ä»»åŠ¡å—ï¼Œæå‡çº¿ç¨‹åˆ©ç”¨ç‡ã€‚
- æ— éœ€çº¿ç¨‹é—´é€šä¿¡ï¼Œå¤©ç„¶é€‚åˆå¹¶è¡Œã€‚

---

#### ğŸ§­ ä¸»èœï¼šç‚¹äº‘éå†æ‰¾æœ€å¤§é«˜åº¦

åœ¨ç‚¹äº‘ç®—æ³•ä¸­ï¼Œéå†å…¨åœºæ‰¾æœ€å¤§å€¼æ˜¯å®¶å¸¸ä¾¿é¥­ï¼Œç”¨ `reduction` ä¼˜é›…æ”¶å°¾ï¼š

```cpp
#pragma omp parallel for reduction(max:max_val)
for (int i = 0; i < cloud.size(); ++i) {
    if (cloud[i].z > max_val)
        max_val = cloud[i].z;
}
```

##### ğŸ§‚ è¯´æ˜ï¼š
- `reduction(max:...)`ï¼šOpenMP è‡ªåŠ¨å¸®ä½ åšçº¿ç¨‹é—´æœ€å¤§å€¼åˆå¹¶ã€‚
- æ¯”èµ·æ‰‹åŠ¨åŠ é”ï¼Œå¿«ä¸æ­¢ä¸€ç‚¹ç‚¹ã€‚

---

#### ğŸ§® ç”œç‚¹ï¼šç»å…¸çŸ©é˜µä¹˜æ³•å¹¶è¡Œ

çŸ©é˜µä¹˜æ³•æ˜¯æ•°å­¦è®¡ç®—é‡Œçš„è·‘åˆ†å¤§ä½¬ï¼Œä¸‰é‡å¾ªç¯ç»“æ„ï¼Œæ‹¿ä¸‹å®ƒæ€§èƒ½å°±èµ·é£ï¼š

```cpp
#pragma omp parallel for
for (int i = 0; i < M; ++i)
    for (int j = 0; j < N; ++j) {
        float sum = 0.0f;
        for (int k = 0; k < K; ++k)
            sum += A[i][k] * B[k][j];
        C[i][j] = sum;
    }
```

##### ğŸ§‚ è¯´æ˜ï¼š
- æˆ‘ä»¬åªå¹¶è¡Œæœ€å¤–å±‚ `i`ï¼Œç®€å•ç¨³å®šã€‚
- æƒ³å†å‡çº§ï¼Ÿå¯ä»¥å°è¯•ç¼“å­˜ä¼˜åŒ– + SIMD æŒ‡ä»¤ã€‚

---

#### ğŸ”¨ ç¼–è¯‘æ–¹æ³•ï¼ˆGCC or Clangï¼‰

```bash
g++ your_code.cpp -fopenmp -O3 -o run
```

âœ… æ³¨æ„ `-fopenmp` æ˜¯æ‰“å¼€å¹¶è¡Œä¸–ç•Œå¤§é—¨çš„é’¥åŒ™ã€‚

---

#### ğŸ“ˆ æ€§èƒ½æµ‹è¯•å»ºè®®

| æ¨¡å— | æµ‹è¯•æ–¹å¼ | è§‚æµ‹æŒ‡æ ‡ |
|------|----------|----------|
| å›¾åƒå·ç§¯ | å¤§å°ºå¯¸å›¾+é«˜æ–¯æ ¸ | é€Ÿåº¦æå‡å€æ•° |
| ç‚¹äº‘éå† | ä¸Šç™¾ä¸‡ç‚¹ | æœ€å¤§å€¼æå–æ—¶é—´ |
| çŸ©é˜µä¹˜æ³• | æ–¹é˜µä¹˜æ³•ï¼ˆå¦‚1024Ã—1024ï¼‰ | GFLOPS æˆ–è€—æ—¶å¯¹æ¯” |

---

#### ğŸ§  æ€»ç»“ä¸€å¥è¯

> OpenMP å°±åƒä¸€é“é­”æ³•æŒ‡ä»¤èœè°±ï¼šåŠ ç‚¹ `#pragma`ï¼Œè®©ä½ çš„ç¨‹åºä»å•æ ¸å°æ¿å‡³ï¼Œä¸€è·ƒä¸Šå¤šæ ¸æ“‚å°ã€‚

## TBB

### ğŸ© TBB æ˜¯ä»€ä¹ˆï¼Ÿ

Intel Threading Building Blocksï¼ˆTBBï¼‰æ˜¯ Intel å¼€å‘çš„ä¸€ä¸ª **ä»»åŠ¡è°ƒåº¦ + æ•°æ®å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œ + é€šç”¨ç®—æ³•åº“**ï¼Œæ—¨åœ¨è®©ä½ åœ¨ C++ ä»£ç ä¸­è½»æ¾å†™å‡ºå¤šçº¿ç¨‹ç¨‹åºï¼Œ**ä¸ç”¨å…³å¿ƒçº¿ç¨‹åˆ›å»ºã€é”€æ¯ã€è°ƒåº¦ç­‰ä½å±‚çäº‹**ã€‚

ä¸€å¥è¯æ€»ç»“ï¼š

> **â€œæˆ‘ä¸å…³å¿ƒçº¿ç¨‹ï¼Œæˆ‘åªå…³å¿ƒæˆ‘æ‰‹ä¸Šçš„å·¥ä½œï¼ˆtaskï¼‰ã€‚â€**

---

### ğŸ”§ TBB æœ‰ä»€ä¹ˆå‰å®³çš„ï¼Ÿ

| èƒ½åŠ› | è¯´æ˜ | å¯¹æ ‡ |
|------|------|------|
| `parallel_for` | å¹¶è¡Œå¾ªç¯ï¼Œè‡ªåŠ¨ä»»åŠ¡åˆ’åˆ† | OpenMP for |
| `parallel_reduce` | å¹¶è¡Œå½’çº¦ï¼ˆåŠ å’Œã€æœ€å¤§ç­‰ï¼‰ | OpenMP reduction |
| `flow::graph` | å¹¶è¡Œå›¾è°ƒåº¦ï¼ˆè¶…å¼ºï¼‰ | ç±»ä¼¼äº task DAG |
| `concurrent_*` | å¹¶å‘å®¹å™¨ï¼ˆå“ˆå¸Œè¡¨ã€é˜Ÿåˆ—ç­‰ï¼‰ | æ— éœ€åŠ é” |
| `task_group` | ç±»ä¼¼ `std::async` + `future` | `std::thread` å‡çº§ç‰ˆ |

---

### ğŸ± å°èœä¸Šæ¡Œï¼šTBB å¹¶è¡Œç¤ºä¾‹ä¸‰é“

---

#### ğŸ¥— ç¤ºä¾‹ 1ï¼šå¹¶è¡Œ `for` å¤„ç†æ•°ç»„

```cpp
#include <tbb/parallel_for.h>
#include <vector>
#include <iostream>

int main() {
    std::vector<float> arr(1e6, 1.0f);

    tbb::parallel_for(0, (int)arr.size(), [&](int i) {
        arr[i] *= 2.0f; // æ¯ä¸ªå…ƒç´ ä¹˜2
    });

    std::cout << "arr[42] = " << arr[42] << std::endl;
    return 0;
}
```

ğŸ§  TBB è‡ªåŠ¨æŠŠå¾ªç¯æ‹†æˆ taskï¼Œè°ƒåº¦åˆ°å„æ ¸ä¸Šï¼Œ**ä½ åªå†™ for ä½“å°±è¡Œ**ï¼Œçº¿ç¨‹æ•°è‡ªå·±è°ƒèŠ‚ã€‚

---

#### ğŸ§® ç¤ºä¾‹ 2ï¼šå¹¶è¡ŒçŸ©é˜µåŠ æ³•

```cpp
#include <tbb/parallel_for.h>
#include <vector>

void mat_add(const std::vector<std::vector<float>>& A,
             const std::vector<std::vector<float>>& B,
             std::vector<std::vector<float>>& C) {
    int M = A.size(), N = A[0].size();
    tbb::parallel_for(0, M, [&](int i) {
        for (int j = 0; j < N; ++j)
            C[i][j] = A[i][j] + B[i][j];
    });
}
```

ğŸ”¥ é€‚åˆå¤§è§„æ¨¡çŸ©é˜µæ“ä½œï¼Œä¸ç”¨æ‰‹åŠ¨æ‹†ä»»åŠ¡ã€‚

---

#### ğŸ§  ç¤ºä¾‹ 3ï¼š`parallel_reduce` å¹¶è¡ŒåŠ å’Œ

```cpp
#include <tbb/parallel_reduce.h>
#include <tbb/blocked_range.h>
#include <vector>
#include <iostream>

int main() {
    std::vector<float> vec(1e6, 1.0f);

    float sum = tbb::parallel_reduce(
        tbb::blocked_range<size_t>(0, vec.size()),
        0.0f,
        [&](tbb::blocked_range<size_t> r, float local_sum) {
            for (size_t i = r.begin(); i < r.end(); ++i)
                local_sum += vec[i];
            return local_sum;
        },
        std::plus<>()
    );

    std::cout << "Sum = " << sum << std::endl;
}
```

â˜• `parallel_reduce` æ˜¯å¹¶è¡ŒåŠ æ€»çš„ä¼˜é›…èŒƒå¼ï¼Œ**å±€éƒ¨å’Œ â†’ åˆå¹¶å™¨ï¼ˆå¦‚ std::plusï¼‰**ã€‚

---

### ğŸ§° ç¼–è¯‘æ–¹å¼ï¼ˆGCC + Intel OneAPI æˆ–ç³»ç»ŸTBBï¼‰

```bash
g++ your_code.cpp -ltbb -O3 -o run
```

---

### ğŸš€ TBB ä¼˜åŠ¿åœ¨å“ªï¼Ÿ

| ç‰¹æ€§ | OpenMP | TBB |
|------|--------|-----|
| ä»»åŠ¡è°ƒåº¦ | é™æ€ã€ç¼–è¯‘æœŸ | åŠ¨æ€ã€è¿è¡Œæ—¶ |
| çµæ´»æ€§ | é«˜ï¼ˆä½†åç»“æ„åŒ–ï¼‰ | **æé«˜**ï¼Œé€‚åˆå¤æ‚ä¾èµ– |
| å¯ç»„åˆæ€§ | å·® | **å¼º**ï¼Œæ”¯æŒåµŒå¥—è°ƒåº¦ |
| æ³›ç”¨æ€§ | ä¸­ç­‰ | **é«˜**ï¼Œæ”¯æŒä»»åŠ¡å›¾ç­‰å¤æ‚æ¨¡å‹ |
| æ”¯æŒå®¹å™¨ | æ—  | `concurrent_vector` ç­‰åŸç”Ÿå¹¶å‘å®¹å™¨ |

---

### ğŸ’¡ åº”ç”¨åœºæ™¯å…¸å‹ç”¨æ³•

- å›¾ç¥ç»ç½‘ç»œå¤šé˜¶æ®µè°ƒåº¦ï¼ˆTBB Flow Graphï¼‰
- è§†è§‰ SLAM ä¸­å¸§å¹¶è¡Œæˆ–åœ°å›¾ä»»åŠ¡è°ƒåº¦
- ç‚¹äº‘å—çº§å¤„ç†ï¼ˆæ¯”å¦‚ voxel gridï¼‰
- å¤§å‹çŸ©é˜µ/å¼ é‡/ç¨€ç–å›¾å¤„ç†
- CPU ç«¯è½»é‡ä»»åŠ¡æµæ°´çº¿

---

### ğŸ§ª Want More?

å¦‚æœä½ æƒ³æ·±å…¥ TBBï¼Œå¯ä»¥ä»è¿™äº›å¼€å§‹ï¼š

- `tbb::flow::graph`ï¼šæ„å»ºå¹¶è¡Œä»»åŠ¡å›¾ï¼Œåƒå†™ç¥ç»ç½‘ç»œé‚£æ ·è°ƒåº¦ä»»åŠ¡
- `tbb::task_arena`ï¼šä»»åŠ¡éš”ç¦»ä¸ç»‘å®šçº¿ç¨‹
- `tbb::concurrent_hash_map`ï¼šçº¿ç¨‹å®‰å…¨å®¹å™¨ï¼Œåª²ç¾ Java ConcurrentHashMap
- è‡ªå®šä¹‰ä»»åŠ¡è°ƒåº¦å™¨ç­–ç•¥ï¼ˆæ¯”å¦‚ affinityï¼‰

## C++ thread
é‚£å°±æŠŠè€ä¸‰è¯·ä¸Šå°ï¼š**C++ Standard Thread Library**ï¼Œä¹Ÿå°±æ˜¯ `std::thread` â€”â€” **çº¿ç¨‹ç•Œçš„åŸæ•™æ—¨ä¸»ä¹‰è€…**ï¼Œä¸åƒ OpenMP å’Œ TBB é‚£æ ·æœ‰èŠ±é‡Œèƒ¡å“¨çš„å®æˆ–è°ƒåº¦å™¨ï¼Œå®ƒå°±æ˜¯ï¼š

> â€œä½ è¦å‡ æ ¹çº¿ç¨‹ï¼Œæˆ‘ç»™ä½ å¼€å‡ æ ¹ã€‚â€

ğŸŒªï¸ ç®€å•ç²—æš´ã€ç¡¬æ ¸ç›´ç™½ï¼Œé€‚åˆä½ æƒ³äº²æ‰‹æ“åˆ€å¤šçº¿ç¨‹è°ƒåº¦çš„åœºæ™¯ï¼Œ**åŒæ—¶ä¹Ÿå®¹æ˜“å†™å‡ºçº¿ç¨‹åœ°ç‹±ï¼ˆthread hellï¼‰**ã€‚

---

### ğŸ› ï¸ `std::thread` æ˜¯ä»€ä¹ˆï¼Ÿ

æ˜¯ C++11 å¼•å…¥çš„æ ‡å‡†åº“ç»„ä»¶ï¼Œç”¨äºç›´æ¥åˆ›å»ºå’Œæ§åˆ¶çº¿ç¨‹ã€‚ä½ å¯ä»¥ï¼š

- å¯åŠ¨æ–°çº¿ç¨‹è·‘å‡½æ•°
- ä¼ å‚
- ç­‰å¾…çº¿ç¨‹ï¼ˆ`join`ï¼‰
- åˆ†ç¦»çº¿ç¨‹ï¼ˆ`detach`ï¼‰
- é…åˆ `mutex`, `condition_variable`, `future` ç­‰å†™æ›´å¤æ‚çš„åŒæ­¥

---

### ğŸ§ª åŸºæœ¬ç”¨æ³•ç¤ºä¾‹ä¸€è§ˆ

#### ğŸŒ± ç¤ºä¾‹ 1ï¼šå¯åŠ¨ä¸€ä¸ªçº¿ç¨‹

```cpp
#include <thread>
#include <iostream>

void hello() {
    std::cout << "Hello from thread!\n";
}

int main() {
    std::thread t(hello);
    t.join(); // ç­‰å¾…çº¿ç¨‹ç»“æŸ
}
```

---

#### ğŸ ç¤ºä¾‹ 2ï¼šå¤šçº¿ç¨‹åŠ é€Ÿå¾ªç¯å¤„ç†

```cpp
#include <thread>
#include <vector>
#include <iostream>

void double_range(std::vector<float>& arr, int start, int end) {
    for (int i = start; i < end; ++i)
        arr[i] *= 2.0f;
}

int main() {
    std::vector<float> arr(1e6, 1.0f);
    int mid = arr.size() / 2;

    std::thread t1(double_range, std::ref(arr), 0, mid);
    std::thread t2(double_range, std::ref(arr), mid, arr.size());

    t1.join();
    t2.join();

    std::cout << "arr[42] = " << arr[42] << std::endl;
}
```

ğŸ§  `std::ref(arr)` æ˜¯é‡ç‚¹ â€”â€” ä½ å¾—æ˜¾å¼å‘Šè¯‰çº¿ç¨‹ä¼ çš„æ˜¯å¼•ç”¨ï¼Œä¸ç„¶å°±å¤åˆ¶è¿‡å»äº†ï¼

---

#### ğŸŒŸ ç¤ºä¾‹ 3ï¼šfuture + async è‡ªåŠ¨çº¿ç¨‹æ± 

```cpp
#include <future>
#include <iostream>

int compute() {
    return 6 * 7;
}

int main() {
    std::future<int> result = std::async(std::launch::async, compute);
    std::cout << "Result: " << result.get() << std::endl;
}
```

ğŸ”® ç”¨ `std::async` å¯ä»¥è‡ªåŠ¨å¯çº¿ç¨‹ï¼Œç®¡ç†ç”Ÿå‘½å‘¨æœŸï¼Œè¿˜èƒ½å¾—åˆ°ç»“æœï¼Œ**è½»é‡åˆé«˜ç«¯**ã€‚

---

### ğŸ’¬ ä¼˜åŠ¿ä¸å±€é™ï¼Ÿ

| ç‰¹æ€§ | `std::thread` |
|------|----------------|
| æ§åˆ¶åŠ› | âœ… ç»å¯¹è‡ªç”±ï¼Œä½ æŒæ¡ä¸€åˆ‡ |
| å­¦ä¹ æ›²çº¿ | ğŸš¨ æœ‰ç‚¹é™¡ï¼Œè¦æ‰‹åŠ¨ç®¡ç†ç”Ÿå‘½å‘¨æœŸã€åŒæ­¥ |
| é€‚åˆåœºæ™¯ | âœ… å°è§„æ¨¡çº¿ç¨‹ã€ç³»ç»Ÿçº§åˆ«ä»»åŠ¡ã€æ•™å­¦ |
| ä¸é€‚åˆ | ğŸš« å¤§è§„æ¨¡å¹¶è¡Œå¤„ç†ï¼ˆä½ ä¸æƒ³è‡ªå·±å†™çº¿ç¨‹æ± ï¼‰ |
| æ¨èç”¨æ³• | âœ… `std::async` + `future` ç»„åˆç”¨ |
| çº¿ç¨‹æ± ï¼Ÿ | âŒ åŸç”Ÿæ²¡æœ‰ï¼Œå¾—ç”¨ç¬¬ä¸‰æ–¹åº“æˆ–è‡ªå·±å†™ |

---

### ğŸ§  ä»€ä¹ˆåœºæ™¯é€‚åˆ `std::thread`ï¼Ÿ

- å†™ä¸€ä¸ªçº¿ç¨‹ç›‘å¬é”®ç›˜/ç½‘ç»œäº‹ä»¶
- ç®€å•åŒçº¿ç¨‹åŠ é€Ÿï¼ˆæ¯”å¦‚ SLAM åŒçº¿ç¨‹ä¼˜åŒ–+è·Ÿè¸ªï¼‰
- æ§åˆ¶ç²’åº¦å¾ˆç»†çš„å¤šçº¿ç¨‹ç³»ç»Ÿ
- å®ç°è‡ªå·±çš„ä»»åŠ¡è°ƒåº¦å™¨æˆ–çº¿ç¨‹æ± 

---

### ğŸ”© C++ çº¿ç¨‹ç”Ÿæ€è¡¥å…¨é€‰é¡¹

| ç”¨é€” | æ¨èåº“ |
|------|--------|
| çº¿ç¨‹æ±  | `BS::thread_pool`, `ctpl`, `asio::thread_pool` |
| å¹¶å‘é˜Ÿåˆ— | `moodycamel::ConcurrentQueue` |
| åç¨‹é£æ ¼ | `C++20 coroutine`, `libgo`, `cppcoro` |
| Actor æ¨¡å‹ | `CAF`, `SObjectizer` |
| åˆ†å¸ƒå¼çº¿ç¨‹æ±  | `Ray`, `HPX` |

---

### ğŸ“¦ ç¼–è¯‘æ–¹æ³•

```bash
g++ my_thread.cpp -std=c++11 -pthread -O2 -o run
```

ä¸€å®šè®°å¾— `-pthread`ï¼Œä¸ç„¶ä½ ä¼šæ”¶åˆ°â€œçº¿ç¨‹ä¸å­˜åœ¨â€çš„æ¸©æŸ”é—®å€™ ğŸ§¨

---

### ğŸš€ æ€»ç»“ï¼ˆé‡‘å¥è­¦å‘Šï¼‰

> `std::thread` æ˜¯ä¸€æŠŠå¼€å±±åˆ€ï¼Œé€‚åˆä½ ä»é›¶æ­å»ºå¹¶å‘ç³»ç»Ÿã€‚  
> ä½†å¦‚æœä½ åªæƒ³ç‚’èœï¼Œ**OpenMP æ˜¯ç”µç£ç‚‰ï¼ŒTBB æ˜¯æ–™ç†æœºï¼ŒThread æ˜¯åŠˆæŸ´åˆ€**ã€‚

## CUDA

CUDAï¼ˆCompute Unified Device Architectureï¼‰â€”â€”è¿™åå­—å¬èµ·æ¥å°±åƒæ˜¯â€œæ˜¾å¡ç•Œçš„é»‘é­”æ³•â€ï¼Œå®åˆ™æ˜¯ **NVIDIA å¼€å‘çš„ä¸€ç§å¹¶è¡Œè®¡ç®—å¹³å°ä¸ç¼–ç¨‹æ¨¡å‹**ï¼Œè®©æˆ‘ä»¬å¯ä»¥ç”¨ C/C++ å†™å‡ºè·‘åœ¨ GPU ä¸Šçš„ä»£ç ã€‚

ç”¨å¤§ç™½è¯è¯´ï¼š  
> **CUDA æ˜¯ä½ æå‡ºæ˜¾å¡é‡Œå‡ åƒä¸ªæ ¸å¿ƒæ¥è·‘å¹¶è¡Œä»»åŠ¡çš„é’¥åŒ™ã€‚**

---

### ğŸ”© CUDA æ˜¯å¹²å•¥çš„ï¼Ÿ

- æŠŠæ•°æ®æ‰”åˆ° GPU
- å¹²ä¸€å †å¹¶è¡Œè¿ç®—ï¼ˆæ¯”å¦‚å›¾åƒå¤„ç†ã€æ·±åº¦å­¦ä¹ ã€çŸ©é˜µä¹˜æ³•ã€ç²’å­æ¨¡æ‹Ÿï¼‰
- æŠŠç»“æœæ‹½å›æ¥

ä½ å†™ä¸ª kernel å‡½æ•°ï¼Œæ˜¾å¡ä¸Šå‡ åƒä¸ªçº¿ç¨‹å°±èƒ½åŒæ—¶æ‰§è¡Œå®ƒï¼Œ**å ªç§°ç§‘å­¦è®¡ç®—ç•Œçš„é£ç«è½®ğŸŒ€**

---

### âš™ï¸ CUDA ç¼–ç¨‹æ¨¡å‹å¿«è§ˆ

```cpp
__global__ void add(int *a, int *b, int *c) {
    int idx = threadIdx.x;
    c[idx] = a[idx] + b[idx];
}
```

```cpp
int main() {
    int a[5] = {1, 2, 3, 4, 5};
    int b[5] = {10, 20, 30, 40, 50};
    int c[5];

    int *d_a, *d_b, *d_c;
    cudaMalloc(&d_a, 5 * sizeof(int));
    cudaMalloc(&d_b, 5 * sizeof(int));
    cudaMalloc(&d_c, 5 * sizeof(int));

    cudaMemcpy(d_a, a, 5 * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, 5 * sizeof(int), cudaMemcpyHostToDevice);

    add<<<1, 5>>>(d_a, d_b, d_c);  // <<<blocks, threads>>>

    cudaMemcpy(c, d_c, 5 * sizeof(int), cudaMemcpyDeviceToHost);

    for (int i = 0; i < 5; ++i)
        std::cout << c[i] << " ";
    std::cout << "\n";

    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);
}
```

---

### âœ¨ è§£é‡Šä¸€ä¸‹é‡Œé¢çš„é»‘è¯ï¼š

| ä¸œè¥¿ | æ˜¯å•¥æ„æ€ |
|------|----------|
| `__global__` | å£°æ˜å‡½æ•°æ˜¯ GPU ä¸Šè·‘çš„ kernel |
| `cudaMalloc` | åœ¨ GPU ä¸Šåˆ†é…å†…å­˜ |
| `cudaMemcpy` | ä¸»æœº â†” è®¾å¤‡ å¤åˆ¶æ•°æ® |
| `add<<<1, 5>>>` | å¯åŠ¨ 1 ä¸ª blockï¼Œæ¯ block å« 5 ä¸ªçº¿ç¨‹ |
| `threadIdx.x` | å½“å‰çº¿ç¨‹åœ¨çº¿ç¨‹å—å†…çš„ç´¢å¼• |
| `cudaFree` | æ˜¾å¡ä¸Šé‡Šæ”¾å†…å­˜èµ„æº |

---

### ğŸš€ ä½ èƒ½ç”¨ CUDA åšå•¥ï¼Ÿ

| åœºæ™¯ | ç”¨æ³• |
|------|------|
| å›¾åƒå¤„ç† | ç°åº¦åŒ–ã€æ»¤æ³¢ã€å·ç§¯ |
| 3D ç‚¹äº‘ | å¤§è§„æ¨¡ç‚¹é‚»åŸŸæŸ¥æ‰¾ã€Voxel Grid |
| æ•°å€¼è®¡ç®— | ç¨€ç–çŸ©é˜µè¿ç®—ã€FFTã€æ±‚å¯¼ |
| æ·±åº¦å­¦ä¹  | Tensor æ“ä½œã€åå‘ä¼ æ’­ï¼ˆPyTorch èƒŒåå°±æ˜¯ CUDAï¼‰ |
| SLAM | Dense Mappingã€Scan Matching |

---

### ğŸ’¡ CUDA å¼€å‘è€…çš„ daily life

ä½ å¾—å…³å¿ƒï¼š

- å†…å­˜è®¿é—®æ˜¯å¦ coalescedï¼ˆåˆå¹¶è®¿é—®å¿«ï¼‰
- æ¯ä¸ª thread åšä»€ä¹ˆï¼Œblock çš„åˆ’åˆ†æ€ä¹ˆåˆ†
- æ˜¾å­˜ copy æˆæœ¬æ˜¯å¦å€¼å¾—
- bank conflictã€warp divergenceï¼ˆçº¿ç¨‹è·‘å¾—ä¸æ•´é½å°±æµªè´¹ï¼‰
- æ˜¯å¦ç”¨ shared memory åšç¼“å­˜ï¼ˆå’Œ L1 ç±»ä¼¼ï¼‰

è¿™æ˜¯â€œæ•°æ®å¹¶è¡Œâ€å’Œâ€œç¡¬ä»¶å‹å¥½â€çš„è‰ºæœ¯ âœ¨

---

### ğŸ”§ ç¼–è¯‘æ–¹å¼ï¼š

```bash
nvcc my_cuda_program.cu -o my_prog
```

ä½ å°±èƒ½å¾—åˆ°ä¸€ä¸ª GPU åŠ é€Ÿçš„å¯æ‰§è¡Œæ–‡ä»¶å•¦ï¼

---

### ğŸ§  å­¦ CUDA éœ€è¦å‡†å¤‡ç‚¹å•¥ï¼Ÿ

| é¡¹ç›® | æ¨è |
|------|------|
| ç¼–ç¨‹è¯­è¨€ | ç†Ÿæ‚‰ C/C++ |
| åŸºç¡€çŸ¥è¯† | å¹¶è¡Œè®¡ç®—æ¨¡å‹ã€å†…å­˜å¸ƒå±€ã€çº¿ç¨‹ |
| å·¥å…·é“¾ | NVIDIA æ˜¾å¡ã€é©±åŠ¨ã€CUDA Toolkit |
| IDE æ’ä»¶ | VSCode + Nsight / CLion + CMake |
| æ•™ç¨‹æ¨è | [CUDA by Example](https://developer.nvidia.com/cuda-example) + NVIDIA å®˜æ–¹æ–‡æ¡£ |

---

### ğŸ§Š å°ç»“ï¼ˆå½©è›‹ï¼‰

> CUDA å°±æ˜¯ã€Œè®©ä½ çš„æ˜¾å¡ä¸ä»…ç©æ¸¸æˆï¼Œè¿˜èƒ½å·æ•°å­¦ã€ã€‚

ä½ è¦æ˜¯ç ”ç©¶è®¡ç®—æœºè§†è§‰ / SLAM / æ·±åº¦å­¦ä¹ æ–¹å‘ï¼Œ**æ—©æ™šéƒ½å¾—å’Œå®ƒç§°å…„é“å¼Ÿ**ã€‚

## OpenCL

æ¥äº†æ¥äº†ï¼ç°åœ¨ä¸Šåœºçš„æ˜¯çœŸæ­£çš„â€œå…¨å¹³å°ç¡¬æ ¸å¼‚æ„è®¡ç®—é»‘é­”æ³•â€â€”â€”

> ğŸ§¬ **OpenCLï¼ˆOpen Computing Languageï¼‰**

---

### ğŸ§  OpenCL æ˜¯ä»€ä¹ˆï¼Ÿ

OpenCL æ˜¯ä¸€ç§ **è·¨å¹³å°çš„å¹¶è¡Œè®¡ç®—æ¡†æ¶**ï¼Œç”± Khronos Groupï¼ˆä¹Ÿå°±æ˜¯ OpenGL é‚£å¸®äººï¼‰ä¸»å¯¼å¼€å‘ï¼Œç›®æ ‡æ˜¯ï¼š

> **åœ¨ä»»ä½•å¹³å°ä¸Šï¼Œè®© CPUã€GPUã€FPGAã€DSP éƒ½èƒ½ä¸€èµ·æ‰“å·¥ã€‚**

å®ƒä¸åƒ CUDA åªèƒ½ç”¨ NVIDIA æ˜¾å¡ï¼ŒOpenCL å¯ä»¥è·‘åœ¨ï¼š

- AMD GPU
- Intel GPU & CPU
- NVIDIAï¼ˆæ”¯æŒä½†ä¸çƒ­æƒ…ï¼‰
- ARM èŠ¯ç‰‡
- Apple M ç³»åˆ—èŠ¯ç‰‡ï¼ˆmacOS ä¸Šæ˜¯äº²å„¿å­ï¼‰

ğŸŒ OpenCL = çœŸÂ·å†™ä¸€ä»½ä»£ç ï¼Œè·‘å…¨ä¸–ç•Œã€‚

---

### ğŸ§ª æ¥ä¸ªç®€å•çš„ä¾‹å­ï¼šçŸ¢é‡åŠ æ³•

#### ğŸ§¾ C++ ä¸»æœºç«¯ä»£ç 

```cpp
#include <CL/cl.h>
#include <iostream>
#include <vector>
#include <fstream>

const char *kernelSrc = R"(
__kernel void add_vec(__global const float* A,
                      __global const float* B,
                      __global float* C) {
    int id = get_global_id(0);
    C[id] = A[id] + B[id];
}
)";

int main() {
    const int N = 1024;
    std::vector<float> A(N, 1.0f), B(N, 2.0f), C(N);

    cl_platform_id platform;
    cl_device_id device;
    cl_context context;
    cl_command_queue queue;

    clGetPlatformIDs(1, &platform, nullptr);
    clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, nullptr);

    context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, nullptr);
    queue = clCreateCommandQueueWithProperties(context, device, 0, nullptr);

    cl_program program = clCreateProgramWithSource(context, 1, &kernelSrc, nullptr, nullptr);
    clBuildProgram(program, 0, nullptr, nullptr, nullptr, nullptr);

    cl_kernel kernel = clCreateKernel(program, "add_vec", nullptr);

    cl_mem d_A = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, sizeof(float)*N, A.data(), nullptr);
    cl_mem d_B = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, sizeof(float)*N, B.data(), nullptr);
    cl_mem d_C = clCreateBuffer(context, CL_MEM_WRITE_ONLY, sizeof(float)*N, nullptr, nullptr);

    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_A);
    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_B);
    clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_C);

    size_t global_work_size = N;
    clEnqueueNDRangeKernel(queue, kernel, 1, nullptr, &global_work_size, nullptr, 0, nullptr, nullptr);

    clEnqueueReadBuffer(queue, d_C, CL_TRUE, 0, sizeof(float)*N, C.data(), 0, nullptr, nullptr);

    std::cout << "C[42] = " << C[42] << std::endl;

    clReleaseMemObject(d_A); clReleaseMemObject(d_B); clReleaseMemObject(d_C);
    clReleaseKernel(kernel); clReleaseProgram(program);
    clReleaseCommandQueue(queue); clReleaseContext(context);
}
```

ğŸ’¡ è¾“å‡ºåº”è¯¥æ˜¯ï¼š`C[42] = 3`

---

### ğŸ”¬ ä¸€äº›æœ¯è¯­è§£é‡Šï¼ˆå’Œ CUDA å¯¹æ¯”ï¼‰

| åŠŸèƒ½ | CUDA | OpenCL |
|------|------|--------|
| GPU å‡½æ•°åä¿®é¥° | `__global__` | `__kernel` |
| çº¿ç¨‹ç´¢å¼• | `threadIdx.x` | `get_global_id(0)` |
| å†…æ ¸è°ƒç”¨ | `<<<blocks, threads>>>` | `clEnqueueNDRangeKernel` |
| å†…å­˜åˆ†é… | `cudaMalloc` | `clCreateBuffer` |
| ä¸»æœºâ†”è®¾å¤‡ä¼ è¾“ | `cudaMemcpy` | `clEnqueueReadBuffer` |
| ç¼–è¯‘å†…æ ¸ | é¢„ç¼–è¯‘ `.cu` | è¿è¡Œæ—¶ç¼–è¯‘æºç å­—ç¬¦ä¸² |

---

### ğŸ’¥ OpenCL çš„ä¼˜ç¼ºç‚¹

#### âœ… ä¼˜ç‚¹

- ğŸ§¬ çœŸÂ·è·¨å¹³å°ï¼ˆWindowsã€Linuxã€macOSã€åµŒå…¥å¼ï¼‰
- ğŸ¦¾ è·¨è®¾å¤‡ï¼ˆCPUã€GPUã€FPGAï¼‰
- ğŸ’¡ å®Œå…¨æ ‡å‡†ï¼Œæ²¡äººèƒ½æŠŠä½ é”æ­»

#### âŒ ç¼ºç‚¹

- ğŸ§± å†™èµ·æ¥æ›´å•°å—¦ï¼ˆå¦‚ä½ æ‰€è§ï¼‰
- ğŸ§© é©±åŠ¨é€‚é…å‘å¤šï¼ˆå°¤å…¶ AMD/NVIDIA çš„ä¸åŒè¡Œä¸ºï¼‰
- ğŸ§  æ‰‹åŠ¨ç®¡ç†å¤ªå¤šèµ„æºï¼Œå®¹æ˜“å†™å‡ºâ€œC å¼ç¾éš¾â€

---

### ğŸ§  ç”¨ OpenCL çš„å…¸å‹åœºæ™¯

- å¼‚æ„è®¡ç®—ä»»åŠ¡ï¼ˆCPU+GPU ååŒï¼‰
- åµŒå…¥å¼å¹³å°ï¼Œå¦‚æ ‘è“æ´¾ã€Jetsonã€æ‰‹æœºèŠ¯ç‰‡
- å­¦æœ¯ç ”ç©¶å¹³å°ï¼Œæƒ³ç”¨ä¸åŒç¡¬ä»¶æµ‹æ€§èƒ½
- æ·±åº¦å­¦ä¹ éƒ¨ç½²ï¼ˆéƒ¨åˆ†æ¡†æ¶å¦‚ TensorFlow æ”¯æŒ OpenCL backendï¼‰

---

### ğŸ”§ ç¼–è¯‘å’Œè¿è¡Œ

éœ€è¦å®‰è£… OpenCL SDKï¼Œä¾‹å¦‚ï¼š
- [Intel OpenCL SDK](https://www.intel.com/content/www/us/en/developer/tools/opencl-sdk/overview.html)
- [NVIDIA CUDA Toolkitï¼ˆå« OpenCL 1.2 æ”¯æŒï¼‰](https://developer.nvidia.com/opencl)
- [POCLï¼ˆCPU Only OpenCL å®ç°ï¼‰](https://portablecl.org/)

ç¼–è¯‘å‘½ä»¤ç¤ºä¾‹ï¼š

```bash
g++ my_opencl.cpp -o run -lOpenCL
```

---

### ğŸ’ æ€»ç»“ä¸€å¥è¯ï¼š

> CUDA æ˜¯æ•ˆç‡è‡³ä¸Šï¼ŒOpenCL æ˜¯è‡ªç”±ä¸‡å²ã€‚

å¦‚æœä½ åå¹³å°æ— å…³ã€è®¾å¤‡é€šåƒã€å…¼å®¹ ARM å’ŒåµŒå…¥å¼ï¼Œé‚£ OpenCL æ˜¯ä½ çš„å¥½å…„å¼Ÿã€‚

## å¹¶è¡Œåº“å¯¹æ¯”
æˆ‘ä»¬ç”¨è¿™ä¸‰æ¿æ–§â€”â€”**æ€§èƒ½ï¼ˆPerformanceï¼‰**ã€**ç”Ÿäº§æ•ˆç‡ï¼ˆProductivityï¼‰**ã€**æ³›ç”¨æ€§ï¼ˆGeneralityï¼‰**ï¼Œæ¥ç»™è¿™äº›ä¸»æµå¹¶è¡Œç¼–ç¨‹åº“æ‰“ä¸ªåˆ†å¹¶åˆ†æå®ƒä»¬èƒŒåçš„**trade-off ç­–ç•¥**ï¼š

---

### ğŸ§¾ è¯„åˆ†ç»´åº¦è¯´æ˜ï¼ˆ0~10ï¼‰ï¼š

| ç»´åº¦ | å«ä¹‰ |
|------|------|
| ğŸ¯ æ€§èƒ½ | ä½å¼€é”€ã€é«˜ååã€è°ƒåº¦æ•ˆç‡ã€cacheå‹å¥½ã€è´Ÿè½½å‡è¡¡ |
| ğŸ’» ç”Ÿäº§æ•ˆç‡ | å­¦ä¹ æ›²çº¿ã€æ˜“ç”¨æ€§ã€æ”¹åŠ¨é‡ã€å·¥å…·é“¾æ”¯æŒ |
| ğŸŒ æ³›ç”¨æ€§ | é€‚åº”ä¸åŒå¹³å°ã€ä¸åŒæ¶æ„ã€æ”¯æŒåœºæ™¯å¹¿åº¦ |

---

### âš”ï¸ å¹¶è¡Œç¼–ç¨‹åº“å…¨æ˜æ˜Ÿè¯„åˆ†å¯¹æ¯”ï¼ˆä¸»è§‚+ä¸»æµå…±è¯†+ç»éªŒï¼‰ï¼š

| åº“ | æ€§èƒ½ ğŸ¯ | ç”Ÿäº§æ•ˆç‡ ğŸ’» | æ³›ç”¨æ€§ ğŸŒ | trade-off ç®€è¿° |
|----|---------|--------------|------------|----------------|
| **OpenMP** | 7 | ğŸ”¥**9** | 7 | æ€§èƒ½ä¸æ˜¯æœ€æè‡´ï¼Œä½†èƒœåœ¨â€œå‡ è¡Œèµ·é£â€ã€‚ç‰ºç‰²è°ƒåº¦è‡ªç”±æ¢æ¥æé«˜çš„ç”Ÿäº§æ•ˆç‡ã€‚ |
| **TBB** | ğŸ”¥**9** | 7 | 8 | ç‰ºç‰²ä¸Šæ‰‹é€Ÿåº¦å’Œä»£ç ç®€æ´åº¦ï¼Œæ¢æ¥äº†æå¼ºçš„è°ƒåº¦èƒ½åŠ›ä¸ç»„åˆèƒ½åŠ›ã€‚ |
| **std::thread / async** | 6 | 6 | ğŸ”¥**9** | ä¸‡é‡‘æ²¹å¼æ³›ç”¨ï¼Œç²’åº¦å¤ªç»†ï¼Œå†™èµ·æ¥ç—›è‹¦ï¼Œè°ƒåº¦å…¨é è‡ªå·±ã€‚æ˜“é”™ä¸”ä¸ scaleã€‚ |
| **Pthreads** | ğŸ”¥**10** | 3 | 8 | æ€§èƒ½å¤©èŠ±æ¿çº§ï¼Œä½†å¼€å‘ä½“éªŒå ªæ¯”ç‚¼ç‹±ã€‚è°ƒåº¦å’ŒåŒæ­¥å…¨æ‰‹æ’¸ã€‚ |
| **CUDA / OpenCL** | ğŸ”¥**10** | 4 | 5 | GPUä¸‹çš„æ€§èƒ½æš´é¾™ï¼Œä½†å¼€å‘æˆæœ¬é«˜ï¼Œæ³›ç”¨æ€§å·®ï¼ˆå¹³å°ä¾èµ–å¤§ï¼‰ã€‚ |
| **HPX** | 8 | 5 | ğŸ”¥**10** | æ³›ç”¨æ€§é¡¶çº§ï¼Œç”šè‡³æ”¯æŒåˆ†å¸ƒå¼å’ŒC++åç¨‹ï¼Œä½†å­¦ä¹ é—¨æ§›é«˜ã€ç”Ÿæ€è¿˜åœ¨æˆé•¿ã€‚ |
| **Kokkos / RAJA** | 8 | 6 | 9 | åœ¨æ€§èƒ½å’Œå¯ç§»æ¤æ€§ä¹‹é—´æ‰“å¹³è¡¡ï¼Œé€‚åˆå¤§ç§‘ç ”é¡¹ç›®ï¼Œè½»é‡å·¥ç¨‹ä¸­åé‡ã€‚ |

---

### ğŸ“‰ å®ƒä»¬éƒ½åšäº†ä»€ä¹ˆ trade-offï¼Ÿ

#### ğŸ§© OpenMP çš„å“²å­¦ï¼š**â€œè®©ä½ 5åˆ†é’Ÿèƒ½è·‘èµ·æ¥ï¼Œåˆ«çº ç»“è°ƒåº¦â€**
- ğŸŒŸ ä¼˜å…ˆçº§ï¼šç”Ÿäº§æ•ˆç‡ ï¼ æ³›ç”¨æ€§ ï¼ æ€§èƒ½
- ğŸŒŒ å®ƒå¸Œæœ›ä½ ä¸€å¼€å§‹å°±èƒ½çœ‹åˆ°æé€Ÿï¼ˆå°¤å…¶åœ¨ç§‘ç ”åœˆã€å·¥ä¸šå°é¡¹ç›®ï¼‰
- ğŸ§¨ ç¼ºç‚¹æ˜¯ä½ æ— æ³• fine-grain æ§åˆ¶ä»»åŠ¡è°ƒåº¦ï¼Œå†™å¤æ‚ç»“æ„å°±ä¸è¡Œäº†

---

#### ğŸ§© TBB çš„å“²å­¦ï¼š**â€œæˆ‘è¦ç»™ä½ ä¸€ä¸ªç°ä»£ã€å¯ç»„åˆçš„å¹¶å‘èŒƒå¼â€**
- ğŸŒŸ ä¼˜å…ˆçº§ï¼šæ€§èƒ½ â‰ˆ æ³›ç”¨æ€§ ï¼ ç”Ÿäº§æ•ˆç‡
- ğŸ§  å¼ºè°ƒâ€œç»“æ„åŒ–å¹¶å‘â€ã€`task_arena`, `flow::graph`ï¼Œä»»åŠ¡é—´ä¾èµ–ã€å¼‚æ­¥ç®¡æ§æ ·æ ·èƒ½æ•´
- ğŸ˜“ ä½†ä»£ä»·æ˜¯ï¼šä½ å¾—ä¼šä¸€ç‚¹ç°ä»£ C++ï¼Œç†è§£ä»»åŠ¡æµå’Œèµ„æºè°ƒåº¦æ¨¡å‹

---

#### ğŸ§© Pthreads / std::thread çš„å“²å­¦ï¼š**â€œæˆ‘å°±æ˜¯çº¿ç¨‹æœ¬ä½“ï¼Œä½ è‡ªç”±å‘æŒ¥â€**
- ğŸŒŸ ä¼˜å…ˆçº§ï¼šæ³›ç”¨æ€§ ï¼ æ€§èƒ½ ï¼ ç”Ÿäº§æ•ˆç‡
- ğŸ› ï¸ å¯¹æå®¢è‡ªç”±ï¼Œä½†æåº¦ç¹çã€å®¹æ˜“è¸©å‘ã€å¾ˆéš¾æ‰©å±•æˆå¯ç»´æŠ¤é¡¹ç›®ç»“æ„

---

#### ğŸ§© CUDA / OpenCL çš„å“²å­¦ï¼š**â€œç‰ºç‰²å¯è¯»æ€§ï¼Œæ¢å–ååç‡â€**
- ğŸŒŸ ä¼˜å…ˆçº§ï¼šæ€§èƒ½ >>> å…¶ä»–
- âš™ï¸ å¼€å‘æ…¢ï¼Œä½†è®¡ç®—å¯†é›†å‹ä»»åŠ¡ä¸Šæ— æ•Œ
- ğŸ’€ æŠ½è±¡å±‚ä½ã€è°ƒåº¦æ¨¡å‹å¤æ‚ï¼Œä¸é€‚åˆé¢‘ç¹å˜æ¢éœ€æ±‚çš„é¡¹ç›®

---

#### ğŸ§© HPX / Kokkos çš„å“²å­¦ï¼š**â€œæœªæ¥ç¼–ç¨‹çš„é›å½¢â€**
- ğŸŒŸ ä¼˜å…ˆçº§ï¼šæ³›ç”¨æ€§ > æ€§èƒ½ > ç”Ÿäº§æ•ˆç‡
- ğŸ”® ä¸ºå¼‚æ„+å¼‚æ­¥+å¤šèŠ‚ç‚¹ç³»ç»ŸæœåŠ¡ï¼Œå­¦æœ¯æ½œåŠ›è‚¡
- ğŸ“š å­¦ä¹ æˆæœ¬å¾ˆé«˜ï¼Œä½†æœªæ¥éšç€ C++20/23 çš„æ™®åŠï¼Œå¯èƒ½ä¼šæ›´å‹å¥½

---

### ğŸ‘‘ æ€»ç»“å›¾åƒè®°å¿†ï¼ˆå¸®ä½ è®°ä½ï¼‰

```
        æ€§èƒ½
          â–²
          |
       ğŸ§  TBB       ğŸ”§ CUDA
        | \         /
        |   \     /
   OpenMP  std::thread
        \     /
        ç®€æ´æ€§
           |
      ğŸ’» ç”Ÿäº§æ•ˆç‡
```

OpenMP æ˜¯é‚£ç§â€œå·¥ç¨‹é‡Œæé€Ÿä¸æ±‚äººâ€çš„è€é“ï¼›  
TBB æ˜¯â€œæ„¿æ„å¤šå†™å‡ è¡Œï¼Œæ¢ä½ ç¨³å®šæ‰©å±•â€ï¼›  
CUDA æ˜¯â€œè€å­å°±æ˜¯è¦è·‘å¾—å¿«â€ï¼›  
HPX æ˜¯â€œæˆ‘ä¸ºå¹¶è¡Œå®‡å®™å¸ƒé“â€ã€‚

## å¤§å®¶æ€ä¹ˆé€‰
å“å˜¿ï¼Œè¿™é—®é¢˜é—®å¾—åƒæäº†â€œç¥ä»™æ‰“æ¶è°ä¸»æ²‰æµ®â€ï¼Œè¿™å°±å¸¦ä½ é€›ä¸€åœˆ**å¹¶è¡Œç¼–ç¨‹æ±Ÿæ¹–**ï¼Œçœ‹çœ‹ TBB å’Œ OpenMP çš„å¯¹æ‰‹ä»¬éƒ½æ˜¯ä»€ä¹ˆæ®µä½ï¼Œè°åˆåœ¨å¼€å‘è€…ä¸­å äº†ä¸»è§’æˆã€‚

---

### ğŸ† é¡¶æµé€‰æ‰‹å¤©æ¢¯æ¦œï¼ˆä¸»æ‰“ C/C++ å…±äº«å†…å­˜å¹¶è¡Œï¼‰ï¼š

| åå­— | ç±»å‹ | èƒŒæ™¯ | ç‰¹ç‚¹ | é€‚åˆäººç¾¤ |
|------|------|------|------|----------|
| **OpenMP** | ç¼–è¯‘å™¨æ‰©å±• API | å¤šå‚å•†è”ç›Ÿ | pragma ä¸Šæ‰‹å¿«ï¼Œä¸»æ‰“å¿«é€Ÿå¹¶è¡ŒåŒ– | æƒ³è½»é‡æé€Ÿçš„å·¥ç¨‹å¸ˆã€ç§‘ç ”äºº |
| **TBB** | C++ åº“ | Intel äº²å„¿å­ | ç°ä»£C++é£æ ¼ã€çµæ´»æ€§é«˜ã€ä»»åŠ¡è°ƒåº¦ç‰› | å–œæ¬¢ç»“æ„æ¸…æ™°ã€é«˜çº§å°è£…çš„å¼€å‘è€… |
| **C++ std::thread / std::async / std::future** | C++æ ‡å‡†åº“ | ISO C++å®˜æ–¹ | åŸç”Ÿæ— ä¾èµ–ï¼Œå†™æ³•åº•å±‚åç¡¬æ ¸ | æƒ³ç©åŸæ•™æ—¨C++çº¿ç¨‹çš„è€å“¥ |
| **Pthreadsï¼ˆPOSIX threadsï¼‰** | ç³»ç»Ÿçº§åº“ | POSIXæ ‡å‡† | åº•å±‚æ§åˆ¶æœ€ç»†ï¼Œä½†å†™èµ·æ¥æœ€çƒ¦ | ç³»ç»Ÿå·¥ç¨‹å¸ˆ / æƒ³åƒè‹¦çš„å‹‡å£« |
| **CUDA / OpenCL** | å¼‚æ„å¹¶è¡Œè®¡ç®— | NVIDIA / Khronos Group | GPUåŠ é€Ÿç¥å™¨ï¼Œçº¿ç¨‹ä¸Šä¸‡èµ·æ­¥ | åšé«˜æ€§èƒ½è®¡ç®—ã€å›¾å½¢ã€AIçš„åŒå­¦ |
| **HPX** | C++ å¼‚æ­¥å¹¶è¡Œæ¡†æ¶ | é«˜æ€§èƒ½è®¡ç®—åœˆ | å…¨å¼‚æ­¥ã€åˆ†å¸ƒå¼å‹å¥½ï¼ŒC++20 åç¨‹åä½œå¥½ | HPCæå®¢ã€æœªæ¥å¹¶è¡Œæ¶æ„å®éªŒå…š |
| **Kokkos / RAJA** | C++å¹¶è¡ŒæŠ½è±¡åº“ | Sandia / LLNL | å±è”½å¹³å°å·®å¼‚ï¼Œä¸€å¥—ä»£ç è·‘CPU/GPU | ç§‘ç ” & è¶…ç®—é¢†åŸŸå¼€å‘è€… |

---

### ğŸ§­ å¼€å‘è€…æ›´å–œæ¬¢è°ï¼Ÿ

è¿™ä¸ªé—®é¢˜è¯´åˆ°åº•è¦çœ‹ä¸‰ä¸ªå­—â€”â€”**åœºæ™¯å…š**ã€‚

#### 1. **OpenMPï¼šä¸­å°è§„æ¨¡å¹¶è¡Œçš„ç¬¬ä¸€é€‰æ‹©**
- å¦‚æœä½ å†™çš„æ˜¯ç§‘ç ”ã€å›¾åƒå¤„ç†ã€ç‚¹äº‘éå†ã€çŸ©é˜µè¿ç®—ã€å›¾åƒå·ç§¯ã€SLAMæ¨¡å—è¿™äº›éœ€è¦**å±€éƒ¨åŠ é€Ÿ**çš„ä¸œè¥¿ï¼›
- å¹¶ä¸”ä½ æƒ³ **ç”¨æœ€å°ä»£ä»·æé€Ÿ** â€”â€”  
âœ… å¤§å¤šæ•°äººé¦–é€‰ OpenMPã€‚

ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºå®ƒï¼š
- ä¸Šæ‰‹å¿«ï¼ˆåŠ ä¸ª `#pragma` å°±è¡Œï¼‰
- æ”¯æŒå¤§éƒ¨åˆ†ç¼–è¯‘å™¨ï¼ˆGCCã€Clangã€MSVC éƒ½æ‡‚ï¼‰
- å¯¹è€ä»£ç å…¼å®¹å¥½ï¼Œæ”¹åŠ¨å°‘

#### 2. **TBBï¼šç°ä»£C++å…šå’Œæ¡†æ¶æ´¾çš„èœ**
- ä½ è¦å†™ä¸ªç°ä»£æ¡†æ¶ï¼Œæˆ–è€…ä½ è¿½æ±‚ **ä»»åŠ¡è°ƒåº¦ä¼˜åŒ–ã€åµŒå¥—å¹¶è¡Œã€ç»„åˆè¡¨è¾¾èƒ½åŠ›**ï¼›
- åˆæˆ–è€…ä½ å†™çš„æ˜¯æ¯”å¦‚å›¾å½¢ç®¡çº¿ã€AIä¸­é—´ä»¶ã€ç¼–è¯‘å™¨å·¥å…·é“¾â€¦â€¦

âœ… é‚£ä½ ä¼šçˆ±ä¸Š TBBã€‚

å®ƒæ¯” OpenMP çµæ´»ï¼Œä»£ç æ›´ä¼˜é›…ï¼Œè€Œä¸”é…åˆ `task_arena`, `concurrent_vector`, `flow::graph` è¿™äº›ç»„ä»¶èƒ½é€ å‡ºä¸€æ•´ä¸ªå¹¶è¡Œå¸å›½ã€‚

#### 3. **GPUå…šèµ°çš„æ˜¯ CUDA / OpenCL è·¯çº¿**
å¦‚æœä½ ç›®æ ‡æ˜¯å¤§è§„æ¨¡å¹¶è¡Œï¼ˆæ¯”å¦‚è·‘ AIã€ç‰©ç†ä»¿çœŸã€æ¸²æŸ“ï¼‰ï¼Œé‚£ï¼š
> OpenMP å’Œ TBB éƒ½æ˜¯å°æ‰“å°é—¹ï¼Œå¾—ä¸Š CUDA / ROCm / oneAPI è¿™ç§æ ¸çˆ†çº§åˆ«çš„ä¸œè¥¿ã€‚

---

### ğŸ¥‡ å¤§ä¼—ä½¿ç”¨ç‡ï¼ˆæ¦‚ç•¥æ„ŸçŸ¥ï¼‰ï¼š

| å¹¶è¡Œåº“ | ä½¿ç”¨ç‡ï¼ˆç§‘ç ” & å·¥ç¨‹åœˆï¼‰ | ä¸»æµç¨‹åº¦ |
|--------|------------------------|----------|
| OpenMP | â­â­â­â­â­ | è¶…ä¸»æµï¼Œæ•™æçº§åº“ |
| TBB | â­â­â­â­ | å·¥ç¨‹ç”¨å¾—å¤šï¼Œç§‘ç ”ç¨å°‘ |
| std::thread | â­â­â­ | å¯ç§»æ¤ä½†ååº•å±‚ |
| Pthreads | â­â­ | ç³»ç»Ÿå¼€å‘ç”¨ |
| CUDA/OpenCL | â­â­â­â­ | GPUåœˆä¸»æµ |
| HPX/Kokkos | â­â­ | è¶…ç®—ç§‘ç ”åœˆæ‰æ‡‚ |

---

### ğŸ’¡ æœªæ¥è¶‹åŠ¿æ€ä¹ˆçœ‹ï¼Ÿ

- **OpenMP** ä¼šç»§ç»­å½“ã€Œå¿«é€Ÿå¹¶è¡ŒåŒ–ã€çš„ç¬¬ä¸€åº“ï¼Œç‰¹åˆ«æ˜¯åœ¨ AI åŠ é€Ÿé¢„å¤„ç†ã€è¾¹ç¼˜è®¡ç®—ã€å›¾åƒã€SLAM ç­‰æ–¹å‘ã€‚
- **TBB** åœ¨ oneAPIã€å¼‚æ„è°ƒåº¦ä¸Šçš„å®šä½è¶Šæ¥è¶Šå¼º â€”â€” ä»¥åä½ æƒ³åœ¨ CPU + GPU + FPGA ä¸Šç»Ÿä¸€è°ƒåº¦ï¼Œå®ƒä¼šæ˜¯ä¸ªé‡è¦é½¿è½®ã€‚
- **std::executionï¼ˆC++23ï¼‰** å¼€å§‹æç»Ÿä¸€å¹¶å‘æ¡†æ¶ï¼Œæœªæ¥ä¼šé€æ¸æŒ‘æˆ˜ TBB çš„åœ°ä½ã€‚

---

### ğŸ”® æ‰€ä»¥æ€»ç»“ï¼š
> æƒ³æé€Ÿå°±ä¸Š OpenMPï¼Œ  
> æƒ³åšç»“æ„ç²¾å·§çš„å¹¶è¡Œç³»ç»Ÿä¸Š TBBï¼Œ  
> æƒ³å†² AI/å›¾å½¢å°±ç›´å¥” CUDAï¼Œ  
> æƒ³åšå…ˆé”‹å®éªŒå¯ä»¥è¯•è¯• HPXã€Kokkosã€‚
